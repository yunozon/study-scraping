{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [初級] QiitaアドベントカレンダーのURL一覧を取得する  \n",
    "アドベントカレンダーはすっかり年末の風物詩となりました。 Qiitaの「クローラー／Webスクレイピング Advent Calendar 2016」に登録された記事の一覧を取得します。気になるものがあれば読んでみても良いでしょう。  \n",
    "引用 : (https://github.com/orangain/scraping-hands-on/blob/master/exercises.md)  \n",
    "問題  \n",
    "以下のページから25日分の記事のURLとタイトルを取得して表示してください。\n",
    "\n",
    "クローラー／Webスクレイピング Advent Calendar 2016 - Qiita  \n",
    "http://qiita.com/advent-calendar/2016/crawler  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アクセス成功\n"
     ]
    }
   ],
   "source": [
    "url = \"https://qiita.com/advent-calendar/2016/crawler\"\n",
    "\n",
    "# アクセスできるかどうかをチェック\n",
    "res = requests.get(url)\n",
    "time.sleep(3)\n",
    "res.raise_for_status()\n",
    "print(\"アクセス成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1件だけ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "calendar = soup.find(\"table\", class_=\"style-to4auo\")\n",
    "\n",
    "# カレンダーの情報を取得\n",
    "d_list = []\n",
    "calendar_info = calendar.find_all(\"td\", class_=\"style-132bvhh\")\n",
    "for info in calendar_info:\n",
    "    page_url = info.find(\"a\", class_ = \"style-14mbwqe\").get(\"href\")\n",
    "    page_title = info.find(\"a\", class_ = \"style-14mbwqe\").text\n",
    "    d_list.append(\n",
    "        {\n",
    "            \"page_url\": page_url,\n",
    "            \"page_title\": page_title\n",
    "        }\n",
    "    )\n",
    "\n",
    "# 各ページの情報をcsvに保存\n",
    "df = pd.DataFrame(d_list)\n",
    "df.to_csv(\"advent_calendar_2016.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_url</th>\n",
       "      <th>page_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://amacbee.hatenablog.com/entry/2016/12/01...</td>\n",
       "      <td>scrapy-splashを使ってJavaScript利用ページを簡単スクレイピング</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://qiita.com/Azunyan1111/items/9b3d16428d...</td>\n",
       "      <td>Python Webスクレイピング 実践入門</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://blog.takuros.net/entry/2016/12/05/082533</td>\n",
       "      <td>非エンジニアでも何とか出来るクローラー／Webスクレイピング術</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://qiita.com/hanaken_Nirvana/items/fc1505...</td>\n",
       "      <td>Scrapy&amp;Twitter Streaming APIを使ったTweetのクローリング</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://qiita.com/checkpoint/items/0c8ad814c25...</td>\n",
       "      <td>Scrapy入門（３）</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://qiita.com/rllllho/items/cb1187cec0fb17...</td>\n",
       "      <td>クローラ作成に必須！XPATHの記法まとめ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://qiita.com/massa142/items/a48e2deb09bca...</td>\n",
       "      <td>tseを使って未投稿があるQiita Advent Calendarをさらす</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://qiita.com/hotu_ta/items/592f751044ed92...</td>\n",
       "      <td>Selenium Builderでスクレイピング/クローラー入門・実践</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://blog.mudatobunka.org/entry/2016/12/18/2...</td>\n",
       "      <td>Scrapy+AWS LambdaでWeb定点観測のイレギュラーに立ち向かう</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://qiita.com/TakesxiSximada/items/39b905e...</td>\n",
       "      <td>Pythonのseleniumライブラリからphantomjsを使ったらzombieになった</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://anoninoni.hateblo.jp/entry/2016/12/12/0...</td>\n",
       "      <td>AWS上にサーバレスな汎用クローラを展開するぞ。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://happyou-info.hatenablog.com/entry/2016/...</td>\n",
       "      <td>中華人民共和国大使館のスクレイピング</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://qiita.com/Hassan/items/ca55b84a093dd69...</td>\n",
       "      <td>Twitter Streaming APIを使った【夢】のクローリング</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://blog.takuros.net/entry/2016/12/25/173900</td>\n",
       "      <td>Pythonクローラー本の決定版か！？　『Pythonクローリング&amp;スクレイピング』</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://orangain.hatenablog.com/entry/duktape</td>\n",
       "      <td>PhantomJSとか使わずに簡単なJavaScriptを処理してスクレイピング</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://qiita.com/imunew/items/0786fd5c9255d4c...</td>\n",
       "      <td>Scrapy Cloudでスクレイピングした成果物をS3にアップロードする</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://blog.takuros.net/entry/2016/12/12/022750</td>\n",
       "      <td>ServerLessで、Amazonのほしい物リストから安売り情報を通知するBotを作ったよ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://qiita.com/yamitzky/items/44a3ea178c750...</td>\n",
       "      <td>mitmproxyを使ってどんなサイトでもクローリング・スクレイピングする</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://qiita.com/mpppk/items/5e8ac21274e9431a...</td>\n",
       "      <td>JavaScriptでブラウザを自動操作できるnightmarejsを使ってガストのクーポン...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://happyou-info.hatenablog.com/entry/2016/...</td>\n",
       "      <td>やはり普及してはならないアンチスクレイピングサービス</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>http://blog.takuros.net/entry/2016/11/18/102815</td>\n",
       "      <td>「データを集める技術」という本を執筆しました</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://blog.takuros.net/entry/2016/10/24/080959</td>\n",
       "      <td>Amazonのほしい物リストをRSS化するAPIを作ってみた</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>http://shinyorke.hatenablog.com/entry/2016/12/...</td>\n",
       "      <td>Pythonを用いたWebスクレイピングの開発ノウハウ〜スポーツデータの場合(野球風味)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             page_url  \\\n",
       "0   http://amacbee.hatenablog.com/entry/2016/12/01...   \n",
       "1   https://qiita.com/Azunyan1111/items/9b3d16428d...   \n",
       "2     http://blog.takuros.net/entry/2016/12/05/082533   \n",
       "3   https://qiita.com/hanaken_Nirvana/items/fc1505...   \n",
       "4   https://qiita.com/checkpoint/items/0c8ad814c25...   \n",
       "5   https://qiita.com/rllllho/items/cb1187cec0fb17...   \n",
       "6   https://qiita.com/massa142/items/a48e2deb09bca...   \n",
       "7   https://qiita.com/hotu_ta/items/592f751044ed92...   \n",
       "8   http://blog.mudatobunka.org/entry/2016/12/18/2...   \n",
       "9   https://qiita.com/TakesxiSximada/items/39b905e...   \n",
       "10  http://anoninoni.hateblo.jp/entry/2016/12/12/0...   \n",
       "11  http://happyou-info.hatenablog.com/entry/2016/...   \n",
       "12  https://qiita.com/Hassan/items/ca55b84a093dd69...   \n",
       "13    http://blog.takuros.net/entry/2016/12/25/173900   \n",
       "14       http://orangain.hatenablog.com/entry/duktape   \n",
       "15  https://qiita.com/imunew/items/0786fd5c9255d4c...   \n",
       "16    http://blog.takuros.net/entry/2016/12/12/022750   \n",
       "17  https://qiita.com/yamitzky/items/44a3ea178c750...   \n",
       "18  https://qiita.com/mpppk/items/5e8ac21274e9431a...   \n",
       "19  http://happyou-info.hatenablog.com/entry/2016/...   \n",
       "20    http://blog.takuros.net/entry/2016/11/18/102815   \n",
       "21    http://blog.takuros.net/entry/2016/10/24/080959   \n",
       "22  http://shinyorke.hatenablog.com/entry/2016/12/...   \n",
       "\n",
       "                                           page_title  \n",
       "0          scrapy-splashを使ってJavaScript利用ページを簡単スクレイピング  \n",
       "1                              Python Webスクレイピング 実践入門  \n",
       "2                     非エンジニアでも何とか出来るクローラー／Webスクレイピング術  \n",
       "3        Scrapy&Twitter Streaming APIを使ったTweetのクローリング  \n",
       "4                                         Scrapy入門（３）  \n",
       "5                               クローラ作成に必須！XPATHの記法まとめ  \n",
       "6              tseを使って未投稿があるQiita Advent Calendarをさらす  \n",
       "7                 Selenium Builderでスクレイピング/クローラー入門・実践  \n",
       "8              Scrapy+AWS LambdaでWeb定点観測のイレギュラーに立ち向かう  \n",
       "9      Pythonのseleniumライブラリからphantomjsを使ったらzombieになった  \n",
       "10                           AWS上にサーバレスな汎用クローラを展開するぞ。  \n",
       "11                                 中華人民共和国大使館のスクレイピング  \n",
       "12                Twitter Streaming APIを使った【夢】のクローリング  \n",
       "13         Pythonクローラー本の決定版か！？　『Pythonクローリング&スクレイピング』  \n",
       "14           PhantomJSとか使わずに簡単なJavaScriptを処理してスクレイピング  \n",
       "15              Scrapy Cloudでスクレイピングした成果物をS3にアップロードする  \n",
       "16     ServerLessで、Amazonのほしい物リストから安売り情報を通知するBotを作ったよ  \n",
       "17              mitmproxyを使ってどんなサイトでもクローリング・スクレイピングする  \n",
       "18  JavaScriptでブラウザを自動操作できるnightmarejsを使ってガストのクーポン...  \n",
       "19                         やはり普及してはならないアンチスクレイピングサービス  \n",
       "20                             「データを集める技術」という本を執筆しました  \n",
       "21                     Amazonのほしい物リストをRSS化するAPIを作ってみた  \n",
       "22       Pythonを用いたWebスクレイピングの開発ノウハウ〜スポーツデータの場合(野球風味)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"advent_calendar_2016.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
